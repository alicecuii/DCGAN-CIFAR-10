{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj2cK8s1CyCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import sys\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdUeHty-hKym",
        "colab_type": "text"
      },
      "source": [
        "# Generator搭建：\n",
        "1. G中的输入可以采用全连接层但是需要reshape成一个4 dimension的tensor(Batch_size, Height, Width, Channel)\n",
        "2. pooling层改成卷积层，G用的fractional-strided convolutions，就是逆卷积，需要上采样\n",
        "3. 需要BatchNormalization，但是不对G的output layer使用BN\n",
        "4. G的激活函数用的ReLU，output层用的Tanh；\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsxUELAJFUPU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "c1e62c7d-4232-4545-d98b-3b5f832968cb"
      },
      "source": [
        "# build_generator(self)\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(128 * 8 * 8, activation=\"relu\", input_dim=100)) # 8,8,128\n",
        "model.add(Reshape((8, 8, 128)))\n",
        "\n",
        "model.add(UpSampling2D()) # 16,16,128\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "model.add(BatchNormalization(momentum=0.8))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(UpSampling2D()) # 32,32,128\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=3, padding=\"same\")) \n",
        "model.add(BatchNormalization(momentum=0.8))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(Conv2D(3, kernel_size=3, padding=\"same\"))\n",
        "model.add(Activation(\"tanh\"))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "noise = Input(shape=(100,))\n",
        "img = model(noise)        \n",
        "        \n",
        "generator = Model(noise, img)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 8192)              827392    \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2 (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 32, 32, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 32, 32, 3)         1731      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 32, 32, 3)         0         \n",
            "=================================================================\n",
            "Total params: 1,051,267\n",
            "Trainable params: 1,050,883\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQLpqwIq3y7Z",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator搭建：\n",
        "1. D中用全卷积层代替池化层，strided convolutions\n",
        "2. 取消全连接层：D中最后的卷积层可以先flatten然后送入一个sigmoid分类器\n",
        "3. 用batchnorm，但是不对D的input layer使用BN\n",
        "4. 在D中所有层的激活函数都是用LeakyReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um8R9VgZFtYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "efb4b676-8a89-4a0f-ae31-d7d2221ac5e4"
      },
      "source": [
        "# build_discriminator\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=(32,32,3), padding=\"same\")) # 16,16,32\n",
        "model.add(LeakyReLU(alpha=0.2))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))  # 8,8,64\n",
        "model.add(BatchNormalization(momentum=0.8))\n",
        "model.add(LeakyReLU(alpha=0.2))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\")) # 4,4,128\n",
        "model.add(BatchNormalization(momentum=0.8))\n",
        "model.add(LeakyReLU(alpha=0.2))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\")) # 4,4,256\n",
        "model.add(BatchNormalization(momentum=0.8))\n",
        "model.add(LeakyReLU(alpha=0.2))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten()) # 4*4*256\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "img = Input(shape=(32,32,3)) # 输入 （28，28，1）\n",
        "validity = model(img) # 输出二分类结果\n",
        "\n",
        "discriminator = Model(img, validity)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 16, 16, 32)        896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 8, 8, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 4097      \n",
            "=================================================================\n",
            "Total params: 394,305\n",
            "Trainable params: 393,409\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owo2CvKj7dPO",
        "colab_type": "text"
      },
      "source": [
        "优化器和论文上的不同，没有用Adam而是RMSprop，这样效果会更好"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbgxh78wFvHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "651de3a1-de8e-4aa8-a49f-f3dc2dbb7e54"
      },
      "source": [
        "optimizer = keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8)\n",
        "\n",
        "# discriminator\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# The combined model  (stacked generator and discriminator)\n",
        "z = Input(shape=(100,))\n",
        "img = generator(z)\n",
        "validity = discriminator(img)\n",
        "# For the combined model we will only train the generator\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Trains the generator to fool the discriminator\n",
        "combined = Model(z, validity)\n",
        "combined.summary()\n",
        "combined.compile(loss='binary_crossentropy', \n",
        "                 optimizer=optimizer)\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "model_7 (Model)              (None, 32, 32, 3)         1051267   \n",
            "_________________________________________________________________\n",
            "model_8 (Model)              (None, 1)                 394305    \n",
            "=================================================================\n",
            "Total params: 1,445,572\n",
            "Trainable params: 1,050,883\n",
            "Non-trainable params: 394,689\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVb2lRo9GqU5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "80b9ba16-923f-4a27-d31b-aaa0ecbe9f67"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "def sample_images(epoch):\n",
        "    r, c = 5, 10\n",
        "    noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = (0.5 * gen_imgs + 0.5)\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,:])# 注意这里的改变\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"/content/drive/My Drive/images0/%d.png\" % epoch)\n",
        "    plt.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rlvvor5HpTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# path = \"/content/drive/My Drive/cifar-10-python.tar.gz\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ6N28YB48jc",
        "colab_type": "text"
      },
      "source": [
        "把数据集的10个不同标签的图片放到一个10*n维的list arrX里面"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4wTX-hHujqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d09d240-f1e4-4dce-e9df-9f0bcbb51865"
      },
      "source": [
        "batch_size = 32\n",
        "sample_interval = 2000\n",
        "# Load the dataset\n",
        "(X_train,Label_train),(X_test,Label_test)=cifar10.load_data()#(50000, 32, 32, 3)\n",
        "print(\"train data:\",'images:',X_train.shape,\n",
        "      \" labels:\",Label_train.shape)\n",
        "# 归一化 Rescale -1 to 1\n",
        "X_train = X_train / 127.5 - 1.\n",
        "\n",
        "arrX=[[] for i in range(10)]\n",
        "for i in range(len(Label_train)):\n",
        "  if Label_train[i]==0:\n",
        "    arrX[0].append(X_train[i])\n",
        "  elif Label_train[i]==1:\n",
        "    arrX[1].append(X_train[i])\n",
        "  elif Label_train[i]==2:\n",
        "    arrX[2].append(X_train[i])\n",
        "  elif Label_train[i]==3:\n",
        "    arrX[3].append(X_train[i])\n",
        "  elif Label_train[i]==4:\n",
        "    arrX[4].append(X_train[i])\n",
        "  elif Label_train[i]==5:\n",
        "    arrX[5].append(X_train[i])\n",
        "  elif Label_train[i]==6:\n",
        "    arrX[6].append(X_train[i])\n",
        "  elif Label_train[i]==7:\n",
        "    arrX[7].append(X_train[i])\n",
        "  elif Label_train[i]==8:\n",
        "    arrX[8].append(X_train[i])\n",
        "  elif Label_train[i]==9:\n",
        "    arrX[9].append(X_train[i])\n",
        "arrX=np.array(arrX)\n",
        "# Adversarial ground truths\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data: images: (50000, 32, 32, 3)  labels: (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg28J4ao5N9q",
        "colab_type": "text"
      },
      "source": [
        "训练不同标签的图片需要改变image_type的值即可；\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a-9FWzSHL0x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "572f7383-c008-45b0-b144-26154e3430e2"
      },
      "source": [
        "\n",
        "for epoch in range(50001):\n",
        "    # ---------------------\n",
        "    #  Train Discriminator\n",
        "    # ---------------------\n",
        "\n",
        "    # Select a random batch of images\n",
        "    image_type=1\n",
        "    idx = np.random.randint(0, arrX[image_type].shape[0], batch_size) # 随机抽  \n",
        "    imgs = arrX[image_type][idx]\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))# 生成标准的高斯分布噪声\n",
        "\n",
        "    # Generate a batch of new images\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, valid) #真实数据对应标签１\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fake) #生成的数据对应标签０\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # ---------------------\n",
        "    #  Train Generator\n",
        "    # ---------------------\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "    # Train the generator (to have the discriminator label samples as valid)\n",
        "    g_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "    # Plot the progress\n",
        "    if epoch % sample_interval==0:\n",
        "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "    # If at save interval => save generated image samples\n",
        "    if epoch % sample_interval == 0:\n",
        "        sample_images(epoch)\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 5.418295, acc.: 23.44%] [G loss: 0.575614]\n",
            "500 [D loss: 0.854747, acc.: 23.44%] [G loss: 0.856658]\n",
            "1000 [D loss: 0.779795, acc.: 37.50%] [G loss: 0.754063]\n",
            "1500 [D loss: 0.812357, acc.: 43.75%] [G loss: 0.767120]\n",
            "2000 [D loss: 0.647590, acc.: 71.88%] [G loss: 0.692581]\n",
            "2500 [D loss: 0.641821, acc.: 79.69%] [G loss: 0.877559]\n",
            "3000 [D loss: 0.713240, acc.: 40.62%] [G loss: 0.708175]\n",
            "3500 [D loss: 0.789789, acc.: 42.19%] [G loss: 0.746944]\n",
            "4000 [D loss: 0.706017, acc.: 39.06%] [G loss: 0.752689]\n",
            "4500 [D loss: 0.689177, acc.: 64.06%] [G loss: 0.737503]\n",
            "5000 [D loss: 0.686403, acc.: 53.12%] [G loss: 0.791514]\n",
            "5500 [D loss: 0.683004, acc.: 56.25%] [G loss: 0.766238]\n",
            "6000 [D loss: 0.708228, acc.: 48.44%] [G loss: 0.805275]\n",
            "6500 [D loss: 0.675979, acc.: 56.25%] [G loss: 0.771018]\n",
            "7000 [D loss: 0.708185, acc.: 57.81%] [G loss: 0.641927]\n",
            "7500 [D loss: 0.737197, acc.: 48.44%] [G loss: 0.689338]\n",
            "8000 [D loss: 0.700146, acc.: 59.38%] [G loss: 0.712323]\n",
            "8500 [D loss: 0.767745, acc.: 53.12%] [G loss: 0.739299]\n",
            "9000 [D loss: 0.696669, acc.: 51.56%] [G loss: 0.750548]\n",
            "9500 [D loss: 0.727764, acc.: 40.62%] [G loss: 0.794401]\n",
            "10000 [D loss: 0.708636, acc.: 54.69%] [G loss: 0.757537]\n",
            "10500 [D loss: 0.618292, acc.: 64.06%] [G loss: 0.742462]\n",
            "11000 [D loss: 0.702855, acc.: 43.75%] [G loss: 0.754075]\n",
            "11500 [D loss: 0.611516, acc.: 67.19%] [G loss: 0.872190]\n",
            "12000 [D loss: 0.649906, acc.: 62.50%] [G loss: 0.655671]\n",
            "12500 [D loss: 0.669448, acc.: 59.38%] [G loss: 0.739397]\n",
            "13000 [D loss: 0.697135, acc.: 46.88%] [G loss: 0.756915]\n",
            "13500 [D loss: 0.607679, acc.: 67.19%] [G loss: 0.833137]\n",
            "14000 [D loss: 0.763938, acc.: 28.12%] [G loss: 0.786143]\n",
            "14500 [D loss: 0.898966, acc.: 28.12%] [G loss: 0.816424]\n",
            "15000 [D loss: 0.639116, acc.: 67.19%] [G loss: 0.846001]\n",
            "15500 [D loss: 0.659224, acc.: 59.38%] [G loss: 0.806779]\n",
            "16000 [D loss: 0.679637, acc.: 59.38%] [G loss: 0.839911]\n",
            "16500 [D loss: 0.753250, acc.: 48.44%] [G loss: 0.868526]\n",
            "17000 [D loss: 0.657088, acc.: 53.12%] [G loss: 0.837833]\n",
            "17500 [D loss: 0.693113, acc.: 60.94%] [G loss: 0.831533]\n",
            "18000 [D loss: 0.651204, acc.: 62.50%] [G loss: 0.956258]\n",
            "18500 [D loss: 0.609105, acc.: 57.81%] [G loss: 0.854612]\n",
            "19000 [D loss: 0.650849, acc.: 65.62%] [G loss: 1.026154]\n",
            "19500 [D loss: 0.575546, acc.: 75.00%] [G loss: 0.833060]\n",
            "20000 [D loss: 0.661668, acc.: 60.94%] [G loss: 0.958474]\n",
            "20500 [D loss: 0.794240, acc.: 35.94%] [G loss: 0.887151]\n",
            "21000 [D loss: 0.653171, acc.: 65.62%] [G loss: 0.925758]\n",
            "21500 [D loss: 0.545684, acc.: 78.12%] [G loss: 0.987393]\n",
            "22000 [D loss: 0.614853, acc.: 64.06%] [G loss: 0.956541]\n",
            "22500 [D loss: 0.653804, acc.: 65.62%] [G loss: 1.190710]\n",
            "23000 [D loss: 0.616301, acc.: 67.19%] [G loss: 0.973120]\n",
            "23500 [D loss: 0.779106, acc.: 54.69%] [G loss: 1.108397]\n",
            "24000 [D loss: 0.639426, acc.: 65.62%] [G loss: 0.900093]\n",
            "24500 [D loss: 0.689518, acc.: 59.38%] [G loss: 1.012399]\n",
            "25000 [D loss: 0.707961, acc.: 53.12%] [G loss: 1.062248]\n",
            "25500 [D loss: 0.501048, acc.: 75.00%] [G loss: 0.977032]\n",
            "26000 [D loss: 0.511751, acc.: 78.12%] [G loss: 0.949633]\n",
            "26500 [D loss: 0.666892, acc.: 64.06%] [G loss: 1.257289]\n",
            "27000 [D loss: 0.492709, acc.: 84.38%] [G loss: 0.942564]\n",
            "27500 [D loss: 0.767130, acc.: 43.75%] [G loss: 1.197249]\n",
            "28000 [D loss: 0.683454, acc.: 62.50%] [G loss: 0.820276]\n",
            "28500 [D loss: 0.599623, acc.: 67.19%] [G loss: 1.294675]\n",
            "29000 [D loss: 0.529633, acc.: 70.31%] [G loss: 1.077631]\n",
            "29500 [D loss: 0.398934, acc.: 84.38%] [G loss: 1.046860]\n",
            "30000 [D loss: 0.613051, acc.: 68.75%] [G loss: 1.611668]\n",
            "30500 [D loss: 0.574881, acc.: 68.75%] [G loss: 0.961771]\n",
            "31000 [D loss: 0.630232, acc.: 57.81%] [G loss: 1.107507]\n",
            "31500 [D loss: 0.515370, acc.: 76.56%] [G loss: 1.101365]\n",
            "32000 [D loss: 0.548290, acc.: 73.44%] [G loss: 1.248129]\n",
            "32500 [D loss: 0.575285, acc.: 75.00%] [G loss: 1.592744]\n",
            "33000 [D loss: 0.684087, acc.: 64.06%] [G loss: 1.287623]\n",
            "33500 [D loss: 0.615325, acc.: 64.06%] [G loss: 1.378066]\n",
            "34000 [D loss: 0.551229, acc.: 70.31%] [G loss: 1.356309]\n",
            "34500 [D loss: 0.521469, acc.: 75.00%] [G loss: 1.800238]\n",
            "35000 [D loss: 0.441673, acc.: 79.69%] [G loss: 1.137524]\n",
            "35500 [D loss: 0.539120, acc.: 75.00%] [G loss: 1.412446]\n",
            "36000 [D loss: 0.465978, acc.: 76.56%] [G loss: 1.485224]\n",
            "36500 [D loss: 0.541209, acc.: 68.75%] [G loss: 1.493047]\n",
            "37000 [D loss: 0.850004, acc.: 40.62%] [G loss: 1.413306]\n",
            "37500 [D loss: 0.329724, acc.: 92.19%] [G loss: 1.699169]\n",
            "38000 [D loss: 0.454689, acc.: 81.25%] [G loss: 1.821723]\n",
            "38500 [D loss: 0.608438, acc.: 59.38%] [G loss: 1.424819]\n",
            "39000 [D loss: 0.342593, acc.: 90.62%] [G loss: 1.511517]\n",
            "39500 [D loss: 0.349301, acc.: 84.38%] [G loss: 1.718740]\n",
            "40000 [D loss: 0.673222, acc.: 65.62%] [G loss: 1.179459]\n",
            "40500 [D loss: 0.482147, acc.: 81.25%] [G loss: 1.652620]\n",
            "41000 [D loss: 0.186706, acc.: 96.88%] [G loss: 1.159787]\n",
            "41500 [D loss: 0.216625, acc.: 92.19%] [G loss: 2.307334]\n",
            "42000 [D loss: 0.629029, acc.: 64.06%] [G loss: 2.299985]\n",
            "42500 [D loss: 0.249511, acc.: 92.19%] [G loss: 2.110638]\n",
            "43000 [D loss: 0.357171, acc.: 87.50%] [G loss: 1.323459]\n",
            "43500 [D loss: 0.780789, acc.: 53.12%] [G loss: 1.035035]\n",
            "44000 [D loss: 0.580145, acc.: 73.44%] [G loss: 2.407694]\n",
            "44500 [D loss: 0.345521, acc.: 89.06%] [G loss: 2.289278]\n",
            "45000 [D loss: 0.486638, acc.: 79.69%] [G loss: 2.242196]\n",
            "45500 [D loss: 0.267327, acc.: 92.19%] [G loss: 2.748227]\n",
            "46000 [D loss: 0.594826, acc.: 67.19%] [G loss: 1.608978]\n",
            "46500 [D loss: 0.284998, acc.: 89.06%] [G loss: 1.983792]\n",
            "47000 [D loss: 0.397467, acc.: 85.94%] [G loss: 2.554166]\n",
            "47500 [D loss: 0.527917, acc.: 75.00%] [G loss: 2.073143]\n",
            "48000 [D loss: 0.645087, acc.: 68.75%] [G loss: 2.722050]\n",
            "48500 [D loss: 0.735655, acc.: 70.31%] [G loss: 2.242309]\n",
            "49000 [D loss: 0.247704, acc.: 89.06%] [G loss: 2.026650]\n",
            "49500 [D loss: 0.620301, acc.: 62.50%] [G loss: 2.562690]\n",
            "50000 [D loss: 0.470154, acc.: 81.25%] [G loss: 3.122800]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
